\chapter{Materials \& Methods}
This chapter details the computational environment, the synthetic datasets used for validation, and the physiological data acquisition protocol. 
It further describes the preprocessing pipeline and the custom implementation of the TE framework using DVP.

\section{Software Environment and Implementation}
The implementation was carried out fully in the Python programming language. 
The following libraries were utilized to facilitate data processing and analysis:
\begin{itemize}
  \item \texttt{NumPy}: Used for high-performance numerical computations and multi-dimensional array operations \cite{numpy}.
  \item \texttt{Pandas}: Employed for the manipulation and analysis of structured tabular datasets \cite{pandas}.
  \item \texttt{Matplotlib}: Utilized for generating high-quality data visualizations and figures \cite{matplotlib}.
  \item \texttt{NeuroKit2}: A specialized library used for the processing and feature extraction of physiological signals \cite{neurokit2}.
  \item \texttt{Pingouin}: Leveraged for performing rigorous statistical tests and inference \cite{pingouin}.
\item \texttt{SciPy}: Utilized for data ranking and Chi-squared distribution utilities \cite{scipy}.
\end{itemize}

\section{Synthetic signals}
To validate correctness of my TE implementation, appropiate benchmark with usage of synthetic signals was performed.
The following factors have been tested:
\begin{itemize}
  \item Effect of a linear coupling strength
  \item Effect of a signal length
  \item Effect of a noise
  \item Effect of a nonlinear coupling strength
  \item Effect of a confounding variable
\end{itemize}

For this purpose the following models were used:
\begin{itemize}
  \item Bivariate linear model
  \item Bivariate nonlinear model
  \item Trivariate linear model
\end{itemize}

Each synthetic dataset was created using $100$ repetitions of given model generation with given parameters.
For reproducibility of the results and to eliminate the effect of randomnes (inherited in each of the synthetic signal generation), within given repetition with different parameters, one seed is used, equal to the repetition number (i.e. generating AR(1) signal at repetition 1 for varying $a$, result in the equal $\epsilon$ for each of them).
The default signal length was set to $200$ to match lengths of physiological signals used for this study, 
To ensure the models reached a steady state, an initial burn-in period of $3,000$ samples was generated and subsequently discarded for each synthetic signal creation.

\subsection{Bivariate Linear Model}
For modeling linear coupling $X \to Y$, an $i$-th order autoregressive model (AR(i)) was employed. 
Based on the literature (\cite{rozo2021benchmarking}; \cite{faes2015information}), the following AR(1) model was implemented:
\begin{equation}
\begin{split}
  x_n &= -0.5 \cdot X_{n-1} + \epsilon_{x_n} \\
  y_n &= -0.5 \cdot x_{n-1} + a \cdot x_{n-1} + \epsilon_{y_n}
\end{split}
\end{equation}

Variable $a$ drives the $X \to Y$ causality, while both $\epsilon_{Y_t}, \epsilon_{X_t}$ are Gaussian white noise processes (mean$=0$, var$=1$).

By default $a$ was set to $0.5$, and the following parametrization was applied: 
\begin{itemize}
  \item $a$ equal: $0, 0.1, 0.25, 0.5$ - to test the effect of a linear coupling strength
  \item Length equal: $100, 200, 500, 1000$ - to test the effect of a signal length
  \item SNR (Signal to Noise Ratio) equal: $\text{None}, 30, 20, 10$ - to test the effect a of noise, where None means noise free
\end{itemize}

\subsection{Bivariate Nonlinear Model}
To verify effectivnes of TE against nonlinear interactions the following model of order 1 from the literature has ben implemented (\cite{rozo2021benchmarking};\cite{lee2012transfer}):
\begin{equation}
\begin{split}
  x_n &= s_{x_n} + \mathcal{L}_{x_n} \\
  y_n &= (bx_{n-1})^2 + \mathcal{L}_{y_n}
\end{split}
\end{equation}

Variable $b$ drives the $X \to Y$ causality, $\mathcal{L}$ is a noise from Laplace distribution (location$=0$, scale$=1$), and s normally distributed noise (mean$=10$, var$=1$).

To verify the effect of different nonlinear strengths, $b$ was set to $0, 0.1, 0.25, 0.5$.

\subsection{Trivariate Linear Model}
To introduce the confounding variable, a following arbitrary linear trivariate AR(1) model is proposed:
\begin{equation}
\begin{split}
  z_n &= 0.8 \cdot z_{n-1} + \epsilon_{z,n} \\
  x_n &= 0.5 \cdot x_{n-1} + a_z \cdot z_{n-1} + \epsilon_{x,n} \\
  y_n &= 0.5 \cdot y_{n-1} + a_x \cdot x_{n-1} + a_z \cdot z_{n-1} + \epsilon_{y,n}
\end{split}
\end{equation}

The parameter $a_z$ is strength of a common driver $Z$ on both $X,Y$, $a_x$ is responsible for direct coupling strength of $X \to Y$, and $\epsilon$ values are adequate Gaussian noises for $X, Y, Z$ (mean$=0$, var$=1$).

To verify the effect of a confounding variable the following parametrization was tested:
\begin{itemize}
  \item $a_z$ set to: $0, 0.1, 0.25, 0.5$ and $a_x=0.3$
  \item $a_x$ set to: $0, 0.1, 0.25, 0.5$ and $a_z=0.4$
\end{itemize}

\section{Physiological signals}
\subsection{Research group}
The data set with the physiological signals come from the study conducted for the \textit{AUTOMATIC} project conducted at Wrocaw University of Wroclaw and Technology (WUST). 
It consists of $32$ young healthy volunteers ($14$ males, $18$ females, median age: $22$ years, range $18$-$31$ years). 
Everyone was instructed not to consume alcohol and caffeine 12 hours before the study and declared to be free of medication. 
Measurements were taken under the supervision of a profesional physician and were approved by the Comission of Bioethics (KB-179/2023/N) \citep{Uryga_2024}.

\subsection{Data acquisition}
For the BRS estimation, mentioned in section \ref{sec:baroreflex}, two raw signals were used among others for the calculations:
\begin{itemize}
  \item ABP recorded noninvasively using a photoplethysmograph (Finometer MIDI)
  \item ETCO$_{2}$ recorded using a capnograph
\end{itemize}

Measurements were made at room temperature with minimal external stimuli as in Figure \ref{fig:experiment-setup} and were carried out in four stages lasting 5 minutes each and performed one after another. 
In stage 1 subjects were breathing at their resting rate, and in stages 2,3,4 they were performing controlled breathing guided by metromone, respectively, at rates 6,10 and 15 breaths per minute (bpm).
Each epoch was performed with a 5 minute long break, the ABP was sampled at $f_s=200 Hz$, and ETCO$_2$ was upsampled to match it.
All the data was gathered into separate \textit{csv} format files, for each patient and breathing stage.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{figs/materials-methods/physiological/experiment_setup.jpg}
    \caption{The experiment setup}
    \label{fig:experiment-setup}
\end{figure}

\subsection{Preprocessing}
Minimal preprocessing was already performed on the received data, which included:
\begin{enumerate}
    \item \textbf{Filtering:}
    \begin{itemize}
        \item A low-pass filter was utilized with cut-off frequency of $40$ Hz to remove high-frequency artefacts
    \end{itemize}

    \item \textbf{Abnormal Peak (Outlier) Detection:}
    \begin{itemize}
        \item Outliers were detected by calculating the signal's first derivative
        \item The derivative was analyzed within a specified window size of $3-11$ samples per window
        \item A peak was classified as an outlier if it exceeded two times the standard deviation of the signal derivative within widnow
    \end{itemize}

    \item \textbf{Outlier Replacement and Missing Value Handling:}
    \begin{itemize}
        \item both outliers and missing values were replaced with a linear interpolation
    \end{itemize}
\end{enumerate}

As the result, signals provided for this study were resembling the following sample in figure \ref{fig:raw-data}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figs/materials-methods/physiological/Preprocessed raw signals.png}
    \caption{Sample of preprocessed raw signals for breathing at rest}
    \label{fig:raw-data}
\end{figure}

\subsection{Signals Processing}
Firstly data is loaded using \textit{BaroreflexDataLoader} that is child of \textit{DataLoader} class.
\begin{minted}{python}
class BaroreflexDataLoader(DataLoader):
  @property
  @override
  def _data_directory(self) -> Path:
      return BREATHING_DATA_DIRECTORY_PATH

  @property
  @override
  def _csv_columns(self) -> dict[str, str]:
      return {'abp': SignalColumns.ABP, 'etco2': SignalColumns.ETCO2}

  @override
  def load_single_subject_raw_data(self, subject_directory: Path) -> SubjectData:
      return {
          'id': self._get_subject_id(subject_directory),
          CB_FILE_TYPE.B6: self.load_single_condition_csv_file(subject_directory / CB_FILE_TYPE.B6.csv),
          CB_FILE_TYPE.B10: self.load_single_condition_csv_file(subject_directory / CB_FILE_TYPE.B10.csv),
          CB_FILE_TYPE.B15: self.load_single_condition_csv_file(subject_directory / CB_FILE_TYPE.B15.csv),
          CB_FILE_TYPE.BASELINE: self.load_single_condition_csv_file(subject_directory / CB_FILE_TYPE.BASELINE.csv),
      }
\end{minted}

The inherited \texttt{load\_single\_subject\_raw\_data()} method handles the whole process of fetching the data from all the CSV files from declared \texttt{\_data\_directory} using \texttt{load\_single\_condition\_csv\_file()}.
Resulting list of \texttt{SubjectData} with loaded ABP and ETCO$_2$ columns for each breathing stage is ready for the processing with usage of \texttt{BaroreflexDataProcessor}.

Similarly as \texttt{BaroreflexDataLoader}, this class inherits methods from the \texttt{DataProcessor} abstract class, that provides ready method for processing signals for all of the subjects and breathing stages for given \texttt{\_process\_single\_cb()} implementation.

\begin{minted}{python}
class BaroreflexDataProcessor(DataProcessor):
    @override
    def _process_single_cb(self, raw_data: ArrayDataDict) -> ArrayDataDict:
        abp = raw_data.get('abp')
        etco2 = raw_data.get('etco2')
        if abp is None or etco2 is None:
            raise ValueError

        peaks = get_peaks(abp)
        sap = get_sap(abp, peaks)
        hp = get_hp(peaks)
        etco2_adjusted = adjust_etco2(etco2, peaks)
        return {'sap': sap, 'hp': hp, 'etco2': etco2_adjusted}
\end{minted}

The end goal of the \texttt{DataProcessor} is to produce the final signals for TE algorithms that are used for the BRS estimation - SAP, HP, ETCO$_2$, using abstracted \texttt{process()} method.
The crucial obstacle at this step is to find realiable way of peaks estimation in the signal.
For this purpose \texttt{NeuroKit2} Python package is leveraged, since it has a collection of ready to use professional algorithms for physiological data processing \cite{neurokit2}.

\begin{minted}{python}
def get_peaks(
    signal: NDArray[np.floating],
    mode: PeaksMode = PeaksMode.UP,
    sampling_rate: int = SAMPLING_FREQUENCY,
    method: str = _DEFAULT_FIND_PEAKS_METHOD,
    mindelay: float = _DEFAULT_MIN_DELAY,
) -> NDArray[np.integer]:
    filled_signal = nk.signal_fillmissing(signal)
    cleaned_signal = cast(NDArray[np.floating], nk.ppg_clean(filled_signal, sampling_rate=sampling_rate, method=method))
    peaks_up: NDArray[np.integer] | None = None
    peaks_down: NDArray[np.integer] | None = None
    if mode in (PeaksMode.UP, PeaksMode.BOTH):
        peaks_up = _find_peaks(
            cleaned_signal,
            sampling_rate=sampling_rate,
            method=method,
            mindelay=mindelay,
        )
    if mode in (PeaksMode.DOWN, PeaksMode.BOTH):
        peaks_down = _find_peaks(
            cleaned_signal * -1,
            sampling_rate=sampling_rate,
            method=method,
            mindelay=mindelay,
        )
    if mode == PeaksMode.BOTH:
        assert peaks_up is not None and peaks_down is not None
        return np.sort(np.concatenate((peaks_up, peaks_down)))
    if mode == PeaksMode.UP:
        assert peaks_up is not None
        return peaks_up
    assert peaks_down is not None
    return peaks_down

def _find_peaks(
    cleaned_signal: NDArray[np.floating],
    sampling_rate: int = SAMPLING_FREQUENCY,
    method: str = _DEFAULT_FIND_PEAKS_METHOD,
    mindelay: float = _DEFAULT_MIN_DELAY,
) -> NDArray[np.integer]:
    peaks = nk.ppg_findpeaks(
        cleaned_signal,
        sampling_rate=sampling_rate,
        method=method,
        mindelay=mindelay,
    )['PPG_Peaks']
    return cast(NDArray[np.integer], peaks)
\end{minted}

There are 3 functions in total used from \texttt{NeuroKit2} (aliased as \texttt{nk}) to get signal peaks in form of array of indecies at which peaks occur, using \textit{"elgendi"} peak detection method by a default. 
While originally developed for photoplethysmography (PPG), this method is highly effective for ABP signals as both share similar pulse morphologies characterized by a rapid systolic rise \cite{elgendi2013systolic}.
\begin{enumerate}
  \item \texttt{nk.signal\_fillmissing()} - fills missing values in signal (what does nothing for this study use case, since missing values were already filled at the preprocessing step).
  \item \texttt{nk.ppg\_clean()} - performs low-pass butterworth filtering (result in Figure \ref{fig:nk-cleaned_signal})
  \item \texttt{nk.ppg\_findpeaks()} - squares positive values of the cleaned signal to amplify the systolic peak energy, then applies a dynamic threshold derived from short- and long-term moving averages to identify segments containing a potential beat. 
    Within these high-energy segments, the most prominent local maximum in the original signal is selected as the final systolic peak (result in Figure \ref{fig:nk-found_peaks}).
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figs/materials-methods/physiological/Cleaned ABP signal.png}
    \caption{Cleaned ABP signal using \texttt{nk.ppg\_clean()}}
    \label{fig:nk-cleaned_signal}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figs/materials-methods/physiological/ABP signal with detected peaks.png}
    \caption{ABP signal with detected peaks using \texttt{nk.ppg\_findpeaks()}}
    \label{fig:nk-found_peaks}
\end{figure}

Having found peaks in the ABP signal, the SAP, HP, and adjusted ETCO$_2$ for each $i$-th $peak$ - which represent indecies in the ABP where they happen, can be calculated using the following formulas:
\begin{equation}
  \text{peaks} = [peak_0, peak_1, \dots, peak_i, \dots]
\end{equation}
\begin{equation}
  \text{SAP}_i = \text{ABP}_{peak_i} \text{ } [mmHg]
\end{equation}
\begin{equation}
  \text{HP}_i = \frac{peak_i - peak_{i-1}}{f_s} \cdot 1000 \text{ } [ms]
\end{equation}
\begin{equation}
  \text{ETCO}_{2i} = \text{mean}(\text{ETCO}_{2peak_{i-1}},\text{ETCO}_{2peak_{i-1}+1}, \dots, \text{ETCO}_{2peak_i}) 
\end{equation}

The above equations correspond to the following code, and are ilustrated in Figure \ref{fig:processing_signals}.

\begin{minted}{python}
def get_sap(abp: NDArray[np.floating], peaks: NDArray[np.integer]) -> NDArray[np.floating]:
    return np.array([abp[peak] for peak in peaks])[1:]  # skip first peak to match length of hp

def get_hp(peaks: NDArray[np.integer], sampling_rate: int = SAMPLING_FREQUENCY) -> NDArray[np.floating]:
    sampling_period = 1 / sampling_rate * 1000
    return np.diff(peaks) * sampling_period

def adjust_etco2(etco2: NDArray[np.floating], peaks: NDArray[np.integer]) -> NDArray[np.floating]:
    if len(etco2) < peaks[-1]:
        raise ValueError('ETCO_2 signal is shorter than peaks!')
    return np.array([np.mean(etco2[peak - 1 : peak]) for i, peak in enumerate(peaks) if i != 0])
\end{minted}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figs/materials-methods/physiological/Preprocessed raw signals, with annotations.png}
    \caption{Processing signals with \texttt{BaroreflexDataProcessor}}
    \label{fig:processing_signals}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figs/materials-methods/physiological/processed signals.png}
    \caption{Processed signals with \texttt{BaroreflexDataProcessor}}
    \label{fig:processed_signals}
\end{figure}

Processed signals like in the Figure \ref{fig:processed_signals} can be easly then be used for TE calculations using the \texttt{BaroreflexResultsGenerator} that just like classes above have abstracted methods for data processing for all the subjects.
\begin{minted}{python}
class BaroreflexResultsGenerator(ResultsGenerator):
    def __init__(self, processed_data: list[SubjectData]) -> None:
        super().__init__(processed_data)

    def add_te(
        self,
        x_name: str,
        y_name: str,
    ) -> str:
        field_name = f'te_{y_name}->{x_name}'
        for subject_id, cb_data_type, cb_data in self.iterate_cb_data():
            x, y = (self._get_signal(cb_data, sig_name, cb_data_type, subject_id) for sig_name in [x_name, y_name])
            try:
                if x is not None and y is not None:
                    self._add_result(
                        condition=cb_data_type,
                        subject_id=subject_id,
                        field_name=field_name,
                        value=te_dv(x, y),
                    )
            except ValueError as e:
                logger.error(f'TE calculation error for P{subject_id} {cb_data_type} {e}')
                self._add_result(
                    condition=cb_data_type,
                    subject_id=subject_id,
                    field_name=field_name,
                    value=None,
                )
        return field_name
\end{minted}

The end goal of this class is to generate CSV file with results for the statistical analysis, with usage of \texttt{generate\_results\_csv()}.
The analysis then are handled by the \texttt{StatisticsAnalyzer} class.

\section{TE with DVP Implementation}\label{sec:te_dvp-implementation}
This section is the most crucial part of this thesis, since it describes all the details concerning TE implementation with DVP for propability estimation.

Since TE is very similar in the implementation to CJTE, the TE workthrough in the following subsections will be used as an example.

\begin{minted}{python}
def te_dv(
    signalX: NDArray[np.floating],
    signalY: NDArray[np.floating],
    time_delay: int = DEFAULT_TIME_DELAY,
    embedding_dimension: int = DEFAULT_EMBEDDING_DIMENSION,
    dvp_alpha=DEFAULT_SIGNIFICANCE_LEVEL,
) -> float:
    """
    Calculates the transfer entropy of TE_{Y->X}
    """
    if len(signalX) != len(signalY):
        logger.error(
            f"""Signals should have the same legth, instead have: \n
            X:{len(signalX)}, Y:{len(signalY)}"""
        )
        raise ValueError('time series entries need to have same length')

    futureX = get_future_vector(signalX, d=embedding_dimension, tau=time_delay)
    pastX, pastY = (get_past_vectors(signal, d=embedding_dimension, tau=time_delay) for signal in [signalX, signalY])

    a = np.column_stack([futureX, pastX, pastY])
    b = np.column_stack([pastX])
    c = np.column_stack([futureX, pastX])
    d = np.column_stack([pastX, pastY])

    dv_result = dv_partition_nd(a, alpha=dvp_alpha)
    if len(dv_result) < MINIMAL_VALID_NUMBER_OF_DV_PARTITONS:
        raise ValueError(
            f'Number of detected bins below minimum: {MINIMAL_VALID_NUMBER_OF_DV_PARTITONS} > {len(dv_result)}'
        )
    dimensions = a.shape[1]
    n_total = a.shape[0]

    futureX_start, futureX_end = 0, 1
    pastX_start, pastX_end = futureX_end, embedding_dimension + 1
    pastY_end = dimensions

    te: float = 0
    for dv_part in dv_result:
        na = dv_part['N']
        nb = get_points_from_range(b, dv_part, ranges=((pastX_start, pastX_end),))
        nc = get_points_from_range(c, dv_part, ranges=((futureX_start, pastX_end),))
        nd = get_points_from_range(d, dv_part, ranges=((pastX_start, pastY_end),))

        te += na / n_total * (np.log2(na * nb) - np.log2(nc * nd))

    return te
\end{minted}

\subsection{Initial Step}
Firstly, let`s take two signals (like in the figure \ref{fig:initial_signals}) for TE computation - $X$ = HP, and $Y$ = SAP, where both of them have to be of length $N=200$.
Also lets use the default time delay $\tau=1$ and embedding dimension $d=1$
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figs/materials-methods/te/Initial Signals.png}
    \caption{Initial Signals}
    \label{fig:initial_signals}
\end{figure}

\subsection{Embedding Signals}

Then we need ordinaly ranked embedding vectors representing future states of $X$ and the past states of both $X$ and $Y$, using equation \ref{eq:embedding-vector}.
These operations are represented by the following functions:

\begin{minted}{python}
def get_past_vectors(signal: FloatArray, d: int, tau: int) -> NDArray[np.integer]:
    embedded = get_deleyed_vector(signal, d=d, tau=tau)
    return np.apply_along_axis(lambda x: rank_transform(x), axis=0, arr=embedded)


def get_deleyed_vector(x: NDArray, d: int, tau: int) -> NDArray:
    n = len(x) - d * tau
    if n <= 0:
        raise ValueError('Time series too short for given embedding.')
    return np.column_stack([x[i - (tau - 1) : i + n - (tau - 1)] for i in range(d * tau - 1, -1, -tau)])


def get_future_vector(signal: NDArray, d: int, tau: int) -> NDArray:
    return rank_transform(signal[d * tau :])


def rank_transform(x: NDArray[np.floating]) -> NDArray[np.integer]:
    return rankdata(x, method=_DEFAULT_RANKING_METHOD)
\end{minted}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figs/materials-methods/te/Embedded Signals.png}
    \caption{Embedded Signals, with $tau=1, d=1$}
\end{figure}

Tweaking $\tau$ parameter to be equal to $2$ results in lag between future and past embeddings is equal to $2$.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figs/materials-methods/te/Embedded Signals, tau=2.png}
    \caption{Embedded Signals, with $tau=2, d=1$}
\end{figure}

Increasing $d$ increases number of embedding dimensions, each lagged by one $\tau$  more.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figs/materials-methods/te/Embedded Signals, d=2.png}
    \caption{Embedded Signals, with $tau=1, d=2$}
\end{figure}

To finish step two, embedding vectors have to be transfered using ordinary ranking:
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figs/materials-methods/te/Ordinary Rank Transformed Signals.png}
    \caption{Ordinary Rank Transformed Vectors}
\end{figure}

\subsection{Probabilities Substitution}
Since we are using DVP alghorithm we have to replace state joint propabilities $p$ from the formula \ref{eq:te-simplified} with probabilities of finding points in given subspaces $a_i,b_i,c_i,d_i$ for $i$-th DVP's space partition. 
\begin{equation}
  TE_{Y\to X}^{d} = \sum p(a_i) \cdot \text{log}(\frac{p(a_i)p(b_i)}{p(c_i)p(d_i)})
\end{equation}
where:
$$ p(x_t,x_{t-1}^{(d)},y_{t-1}^{(d)}) \to p(a_i) $$
$$ p(x_{t-1}^{(d)}) \to p(b_i) $$
$$ p(x_t,x_{t-1}^{(d)}) \to p(c_i) $$
$$ p(x_{t-1}^{(d)},y_{t-1}^{(d)}) \to p(d_i) $$

In fact here the biggest difference between TE  and CJTE lies, since each of them will have different subspace assigment for the incoming signals:
\begin{equation}
  CJTE_{(X,Y)\to Z|Y}^{d} = \sum p(a_i) \cdot \text{log}(\frac{p(a_i)p(b_i)}{p(c_i)p(d_i)})
\end{equation}
where:
$$ p(z_t,z_{t-1}^{(d)},x_{t-1}^{(d)},y_{t-1}^{(d)}) \to p(a_i) $$
$$ p(z_{t-1}^{(d)},y_{t-1}^{(d)}) \to p(b_i) $$
$$ p(z_t,z_{t-1}^{(d)},y_{t-1}^{(d)}) \to p(c_i) $$
$$ p(z_{t-1}^{(d)},x_{t-1}^{(d)},y_{t-1}^{(d)}) \to p(d_i) $$

\subsection{Appling DVP}
The fourth step is to calculate the DVP space partitioning, for the space $a$, (which in case of TE, contains vector embedding future of $X$, and past of $X$, and $Y$). 
\begin{minted}{python}
class DVPartition(TypedDict):
    mins: NDArray[np.integer]
    maxs: NDArray[np.integer]
    N: int

def dv_partition_nd(
    data: NDArray[np.integer],
    mins: NDArray[np.integer] | None = None,
    maxs: NDArray[np.integer] | None = None,
    alpha: float = DEFAULT_SIGNIFICANCE_LEVEL,
) -> list[DVPartition]:
    is_initial = False
    if mins is None or maxs is None:
        is_initial = True
        mins = cast(NDArray[np.integer], np.min(data, axis=0))
        maxs = cast(NDArray[np.integer], np.max(data, axis=0))
    dimensions = len(mins)

    current_box_data = _get_current_box_data(data, mins, maxs)
    n = len(current_box_data)
    if n == 0:
        return []

    children, counts = _get_children_with_counts(current_box_data, mins, maxs)

    is_uniform = _is_uniform(dimensions, counts=counts, alpha=alpha)
    if is_uniform is None:
        return []

    parts = []
    if is_initial or ((not is_uniform) and np.any(maxs - mins)):
        for child_mins, child_maxs in (child for child in children if child is not None):
            parts.extend(dv_partition_nd(data, child_mins, child_maxs, alpha))

        return parts

    # else this box is a leaf
    return [{'mins': mins.copy(), 'maxs': maxs.copy(), 'N': int(n)}]
\end{minted}

The first thing that happens into mine DVP implementation is assigment of the initial borders defined by mins and maxs, if it is the first iteration of the recursion.
Then current (hyper) box data is cutted out of data based on borders.
\begin{minted}{python}
def _get_current_box_data(
    data: NDArray[np.integer],
    mins: NDArray[np.integer],
    maxs: NDArray[np.integer],
) -> NDArray[np.integer]:
    """
    Selects the subset of data points that fall within the current hyper-box.
    """
    inside = np.all((data >= mins) & (data <= maxs), axis=1)
    return data[inside]
\end{minted}

Then the safety check is done if the current box has any points.
If current hyper box contains any points the equal space division along midpoints is performed resulting in children hyper boxes and counts of points inside of them.
\begin{minted}{python}
def _get_children_with_counts(
    current_box_data: NDArray[np.integer],
    mins: NDArray[np.integer],
    maxs: NDArray[np.integer],
) -> tuple[list[tuple[NDArray[np.integer], NDArray[np.integer]] | None], NDArray[np.integer]]:
    midpoints: NDArray[np.integer] = (mins + maxs) / 2  # type: ignore
    dimensions = len(mins)

    children = []
    counts = []
    for bits in product([0, 1], repeat=dimensions):
        child_mins, child_maxs = _get_child_box_bounds(bits, mins, maxs, midpoints)
        count = np.all((current_box_data >= child_mins) & (current_box_data <= child_maxs), axis=1).sum()
        counts.append(count)
        if count == 0:
            children.append(None)
        else:
            children.append((child_mins, child_maxs))

    return children, np.asarray(counts)

def _get_child_box_bounds(
    bits: tuple[int, ...], mins: NDArray[np.integer], maxs: NDArray[np.integer], midpoints: NDArray[np.integer]
) -> tuple[NDArray[np.integer], NDArray[np.integer]]:
    """Calculates mins and maxs for bounds of a child box."""
    # If bit is 0, use mins/midpoints; if bit is 1, use midpoints+1/maxs
    bits_array = np.array(bits)
    child_mins = np.where(bits_array == 0, mins, midpoints + 1)
    child_maxs = np.where(bits_array == 0, midpoints, maxs)
    return child_mins, child_maxs
\end{minted}

Finally, if the space division results in non-uniform points distribution among children, the recursion enters each of the children, else the whole current hyper box is treated as one partition and is returned.
The above rule has the one expection - if this is the initial call of the function, the split is forced, since Chi-squared test often takes the initial distribution as uniform, preventing any recursion, which results in returning just the initial data.
The significance level ($\alpha$) \texttt{alpha} to pass this uniformity test is by default set as $0.05$, what is proposed in the original paper, but my implementation allows for modification of this value \cite{dvp}.

\begin{minted}{python}
def _is_uniform(d: int, counts: NDArray, alpha: float) -> None | bool:
    mean = counts.mean()
    if mean == 0:
        return None
    T = np.sum((mean - counts) ** 2 / mean)
    crit = chi2.ppf(1 - alpha, df=(2**d) - 1)
    return crit >= T
\end{minted}

After all the recursion instances are resolved, the division of space can be plotted as in the Figure \ref{fig:dvp-example-implementation}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figs/materials-methods/te/Darbellay-Vajda Adaptive Partitioning (3D).png}
    \caption{The final space partition, implemented from scratch}
    \label{fig:dvp-example-implementation}
\end{figure}

\subsection{Summation}
The final step in TE (and CJTE) calculation is to count for each $i$-th space partition from DVP alghorithm the partial TE sum.
To get the ammount of points in given subspace the following function is being used:
\begin{minted}{python}
def get_points_from_range(
    points: NDArray[np.integer], dv_part: DVPartition, ranges: tuple[tuple[int, int], ...]
) -> int:
    mins: NDArray[np.integer] | None = None
    maxs: NDArray[np.integer] | None = None

    for start, stop in ranges:
        part_mins, part_maxs = _get_min_max(dv_part, start, stop)

        # Initialize on first loop
        if mins is None or maxs is None:
            mins = part_mins
            maxs = part_maxs
        else:
            mins = np.hstack([mins, part_mins])
            maxs = np.hstack([maxs, part_maxs])

    mins, maxs = cast(NDArray[np.integer], mins), cast(NDArray[np.integer], maxs)
    return np.all((points >= mins) & (points <= maxs), axis=1).sum()
\end{minted}

\section{Complete Workflow}
All the code needed to produce the results included in the following chapter is gathered in a single \texttt{main.py} file. 
Therefore, only the single command \texttt{uv run python main.py} is required, provided that the data is correctly placed in the \texttt{data/} directory. 
Instructions for setting up the environment are provided in the \texttt{README.md} file. 

The complete workflow is summarized in Figure \ref{fig:complete-workflow}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figs/materials-methods/complete-workflow.png}
    \caption{Diagram representing the complete workflow of the implemented code}
    \label{fig:complete-workflow}
\end{figure}

