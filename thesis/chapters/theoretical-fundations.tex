\chapter{Theoretical Foundations}\label{ch:theoretical-fundations}

\section{Baroreflex}\label{sec:baroreflex}
% Czemu te sygna≈Çy, metody (z fizjologii) i co2?

\section{Information Theory}\label{sec:information-theory}
In 1948 an article titled: \textit{"A Mathematical Theory of Communication"} was proposed by Shannon, giving raise to a new discipline called Information Theory \cite{Shannon1948}.

Shannon's main objective was to establish a quantitative measure of information independent of its meaning.
From this foundation, his work provided the methodology to calculate two fundamental limits: the theoretical maximum compression of a message (measured in bits) without losing information it conveys, and the noisy channel capacity, which is the maximum rate at which information can be transmitted at nearly zero error.

The theory contributed fundamentally to fields like statistical physics, computer science, and mathematical statistics, and was also found useful in diverse areas such as economics, quantum mechanics, and in the study of human physiology (\cite{Cover2006};\cite{lossofcomplexity}).

\subsection{Shannon Entropy}
Shannon proposed his own definition of Entropy (called also Shannon Entropy), different from the one in the thermodynamics.

In Information Theory, The Entropy $H(X)$ of a discreate random variable $X$ and propability mass function $p(x) = Pr\{X = x\}, x \in X$ is defined as:
\begin{equation}\label{eq:entropy}
    H(X) = -\sum_{x\in X} p(x) \cdot log(p(x))
\end{equation}
For convenience $p(x)$ notation is used instead of $p_X(x)$, and $log$ over $log_2$.
Thus, $p(x)$ and $p(y)$ refer to two different random variables and are equivalent to $p_X(x)$ and $p_Y(y)$.

The base of $log$ in the formula denotes the unit in which Entropy is calculated, where base 2 stands for bits. 
The resulting number can be interpreted in different ways, based on the use case.
From the information transfer perspective $H(X)$ will mean the maximum theoretical compression of $X$, where in the biological field it will rather be interpreted as total uncertrainity, ammount of information, or complexity of system $X$ (\cite{Cover2006};\cite{Shannon1948};\cite{lossofcomplexity}).

As an example, we can take tossing a fair coin 3 times, denoting its result as $X$. 
In Table \ref{tab:cointossing} we observe that we can have 8 possible outcomes $x$, each of equal probability $p(x)$ equal to $\frac{1}{8}$.
After putting this into formula \ref{eq:entropy} we have:
\begin{equation}
    H(X) = -\sum_{x=1}^{8} p(x) \cdot log(p(x)) = 3\text{ bits}
\end{equation}

\begin{table}[H]
\centering
\caption{All possible combinations of tossing a fair coin 3 times, where H and T denotes respectively heads and tails}
\label{tab:cointossing}
\begin{tabular}{|c|c|c|}
\hline
\textbf{No.} & \textbf{Coin Combination} & \textbf{Probability} \\
\hline
1 & HHH & $\frac{1}{8}$ \\
2 & HHT & $\frac{1}{8}$ \\
3 & HTH & $\frac{1}{8}$ \\
4 & HTT & $\frac{1}{8}$ \\
5 & THH & $\frac{1}{8}$ \\
6 & THT & $\frac{1}{8}$ \\
7 & TTH & $\frac{1}{8}$ \\
8 & TTT & $\frac{1}{8}$ \\
\hline
\end{tabular}
\end{table}

In terms of biological systems improved versions of Shannon Entropy are found to be more effective, such as Fuzzy, Sample, Distribution, or Multiscale Entropy(\cite{Costa2003};\cite{Castiglioni2023}).
Nevertheless the core idea of measuring the ammount of information in the system stays the same.

\subsection{Joint Entropy and Conditional Entropy}\label{subsec:je_ce}
The natural extension of Entropy will be calculating it for a pair of random variables $X$ and $Y$.

The Joint Entropy (JE) $H(X,Y)$ of a pair of discrete random variables ($X$, $Y$) with a joint distribution $p(x,y)$ is defined as:
\begin{equation}\label{eq:joint-entropy}
    H(X,Y) = -\sum_{x\in X}\sum_{y\in Y} p(x,y) \cdot log(p(x,y))
\end{equation}

Instead of calculating Entropy for the joint propability distribution, we can define Entropy over the conditioning variable instead.

The Conditional Entropy (CE) $H(X|Y)$ if ($X$,$Y$) $\sim$ $p(x,y)$ is:
\begin{equation}\label{eq:conditional-entropy}
    H(X|Y) = -\sum_{x\in X}\sum_{y\in Y} p(x,y) \cdot log(p(x|y))
\end{equation}

The relation between CE  and JE  can be expressed using the following chain rule:

\begin{equation}\label{eq:conditional-entropy-chain}
    H(X|Y) = H(X,Y) - H(Y)
\end{equation}

The CE can be interpreted as a reduction of the uncertrainity of $X$ given $Y$, so the better $Y$ helps to predict $X$, the smaller $H(X|Y)$ is (\cite{Shannon1948};\cite{Cover2006}).

\subsection{Transfer entropy}
To measure to which extent information is transfered from individual components in a dynamic and non-determenistic systems, TE was introduced \cite{Schreiber2000}. 

The dynamical structure is needed for TE calculation, rather than static probabilities, as it had place for the previous entropies.
This effect can be achived with introduction of transition probabilities from Markov process.
The Markov random process $X$ of order $k$ has the property, that conditional probability of finding it in state $x_{t}$ at time $t$, is independent of the state $x_{t-k-1}$.
This fact can be expressed with the following equation:
\begin{equation}\label{eq:markov-chain}
  p(x_t|x_{t-1}, \dots, x_{t-k-1}) = p(x_t|x_{t-1}, \dots, x_{t-k})
\end{equation}

In other words - the conditional probability of finding $X$ in state $x_t$ only depends on $k$ last states.
The shorthand notation $x_t^{(k)} = (i_t, \dots, i_{t-k})$ from this point will be used.
To introduce the time delay $\tau$ the following embedding vector of the past states is constructed:
\begin{equation}\label{eq:embedding-vector}
  x_t^{k,\tau} = (x_t, x_{t-\tau}, x_{t-2\tau} \dots, x_{t-k\tau})
\end{equation}

However $\tau$ called also embedding delay in the most cases is set to be equal to 1, therefore is not used in the notation.
The order of markov process noted in equation \ref{eq:embedding-vector} as $k$ is often called embedding dimensions.

Finally $TE_{Y\to X}$ from random process $Y$  to random process $X$, with the corenspoding states $y_t$, $x_t$ and embedding dimensions $l$, $k$ is defined as:
\begin{equation}\label{eq:te}
  TE_{Y\to X}^{k,l} = \sum_{x_t,x_{t-1}^{(k)},y_{t-1}^{(l)}}p(x_t,x_{t-1}^{(k)},y_{t-1}^{(l)}) \cdot log(\frac{p(x_t|x_{t-1}^{(k)},y_{t-1}^{(l)})}{p(x_t|x_{t-1}^{(k)})})
\end{equation}

From the original paper and other scientific publications the embedding dimensions $l$ and $k$ are usually set as $l=k=1$ \cite{Schreiber2000}.

\subsection{Conditional Transfer Entropy and Conditional Joint Transfer Entropy}
As in the subsection \ref{subsec:je_ce} the basic Shannon Entropy can be extended with joints and conditions and the same property is also true for TE to aquire even more powerful measurement tool.

The Conditional Transfer Entropy (CTE) introduces thrid variable $Z$ responsible for conditioning.
This property is crucial in accessing the information transfer from one process to the other excluding the influence of the third factor.

Similarly to the definition \ref{eq:te}  $CTE_{Y\to X|Z}$ is equal to:
\begin{equation}\label{eq:cte}
  CTE_{Y\to X|Z}^{k} = \sum_{x_t,x_{t-1}^{(k)},y_{t-1}^{(k)},z_{t-1}^{(k)}}p(x_t,x_{t-1}^{(k)},y_{t-1}^{(l)},z_{t-1}^{(k)}) \cdot log(\frac{p(x_t|x_{t-1}^{(k)},y_{t-1}^{(k)},z_{t-1}^{(k)})}{p(x_t|x_{t-1}^{(k)},z_{t-1}^{(k)})})
\end{equation}

\section{Estimation techniques}
\cite{genccaga2018transfer}\cite{lee2012transfer}\cite{rozo2021benchmarking}\cite{wen2023kendall}\cite{dvp}

\section{Autonomic nervous system}

