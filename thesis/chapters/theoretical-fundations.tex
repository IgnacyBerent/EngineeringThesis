\chapter{Theoretical Foundations}\label{ch:theoretical-fundations}

\section{Baroreflex}\label{sec:baroreflex}
% Czemu te sygna≈Çy, metody (z fizjologii) i co2?

\section{Information Theory}\label{sec:information-theory}
In 1948 an article titled: \textit{"A Mathematical Theory of Communication"} was proposed by Shannon, giving raise to a new discipline called Information Theory \cite{Shannon1948}.

Shannon's main objective was to establish a quantitative measure of information independent of its meaning.
From this foundation, his work provided the methodology to calculate two fundamental limits: the theoretical maximum compression of a message (measured in bits) without losing information it conveys, and the noisy channel capacity, which is the maximum rate at which information can be transmitted at nearly zero error.

The theory contributed fundamentally to fields like statistical physics, computer science, and mathematical statistics, and was also found useful in diverse areas such as economics, quantum mechanics, and in the study of human physiology (\cite{Cover2006};\cite{lossofcomplexity}).

\subsection{Shannon Entropy}
Shannon proposed his own definition of Entropy (called also Shannon Entropy), different from the one in the thermodynamics.

In Information Theory, The Entropy $H(X)$ of a discreate random variable $X$ and propability mass function $p(x) = Pr\{X = x\}, x \in X$ is defined as:
\begin{equation}\label{eq:entropy}
    H(X) = -\sum_{x\in X} p(x) \cdot log(p(x))
\end{equation}
For convenience $p(x)$ notation is used instead of $p_X(x)$, and $log$ over $log_2$.
Thus, $p(x)$ and $p(y)$ refer to two different random variables and are equivalent to $p_X(x)$ and $p_Y(y)$.

The base of $log$ in the formula denotes the unit in which Entropy is calculated, where base 2 stands for bits. 
The resulting number can be interpreted in different ways, based on the use case.
From the information transfer perspective $H(X)$ will mean the maximum theoretical compression of $X$, where in the biological field it will rather be interpreted as total uncertrainity, ammount of information, or complexity of system $X$ (\cite{Cover2006};\cite{Shannon1948};\cite{lossofcomplexity}).

As an example, we can take tossing a fair coin 3 times, denoting its result as $X$. 
In Table \ref{tab:cointossing} we observe that we can have 8 possible outcomes $x$, each of equal probability $p(x)$ equal to $\frac{1}{8}$.
After putting this into formula \ref{eq:entropy} we have:
\begin{equation}
  H(X) = -\sum_{x=1}^{8} p(x) \cdot log(p(x)) = 3\text{ [bits]}
\end{equation}

\begin{table}[H]
\centering
\caption{All possible combinations of tossing a fair coin 3 times, where H and T denotes respectively heads and tails}
\label{tab:cointossing}
\begin{tabular}{|c|c|c|}
\hline
\textbf{No.} & \textbf{Coin Combination} & \textbf{Probability} \\
\hline
1 & HHH & $\frac{1}{8}$ \\
2 & HHT & $\frac{1}{8}$ \\
3 & HTH & $\frac{1}{8}$ \\
4 & HTT & $\frac{1}{8}$ \\
5 & THH & $\frac{1}{8}$ \\
6 & THT & $\frac{1}{8}$ \\
7 & TTH & $\frac{1}{8}$ \\
8 & TTT & $\frac{1}{8}$ \\
\hline
\end{tabular}
\end{table}

In terms of biological systems improved versions of Shannon Entropy are found to be more effective, such as Fuzzy, Sample, Distribution, or Multiscale Entropy(\cite{Costa2003};\cite{Castiglioni2023}).
Nevertheless the core idea of measuring the ammount of information in the system stays the same.

\subsection{Joint Entropy and Conditional Entropy}\label{subsec:je_ce}
The natural extension of Entropy will be calculating it for a pair of random variables $X$ and $Y$.

The Joint Entropy (JE) $H(X,Y)$ of a pair of discrete random variables ($X$, $Y$) with a joint distribution $p(x,y)$ is defined as:
\begin{equation}\label{eq:joint-entropy}
    H(X,Y) = -\sum p(x,y) \cdot log(p(x,y))
\end{equation}

For the simplification, the following notation was shorthanded with signle $\sum$ notation: $\sum_{x\in X}\sum_{y\in Y}$ and will be kept for the remaining definitions.

Instead of calculating Entropy for the joint propability distribution, we can define Entropy over the conditioning variable instead.

The Conditional Entropy (CE) $H(X|Y)$ if ($X$,$Y$) $\sim$ $p(x,y)$ is:
\begin{equation}\label{eq:conditional-entropy}
    H(X|Y) = -\sum p(x,y) \cdot log(p(x|y))
\end{equation}

The relation between CE  and JE  can be expressed using the following chain rule:

\begin{equation}\label{eq:conditional-entropy-chain}
    H(X|Y) = H(X,Y) - H(Y)
\end{equation}

The CE can be interpreted as a reduction of the uncertrainity of $X$ given $Y$, so the better $Y$ helps to predict $X$, the smaller $H(X|Y)$ is (\cite{Shannon1948};\cite{Cover2006}).

\subsection{Transfer entropy}
To measure to which extent information is transfered from individual components in a dynamic and non-determenistic systems, TE was introduced as a model-free tool (\cite{Schreiber2000};\cite{wen2023kendall}). 

The dynamical structure is needed for TE calculation, rather than static probabilities, as it had place in the previous formulas.
This effect can be achived with the introduction of transition probabilities from Markov process.
The Markov random process $X$ of order $k$ has a property, that conditional probability of finding it in state $x_{t}$ at time $t$, is independent of the state $x_{t-k-1}$.
This fact can be expressed with the following equation:
\begin{equation}\label{eq:markov-chain}
  p(x_t|x_{t-1}, \dots, x_{t-k-1}) = p(x_t|x_{t-1}, \dots, x_{t-k})
\end{equation}

In other words - the conditional probability of finding $X$ in state $x_t$ only depends on $k$ last states.
The shorthand notation $x_t^{(k)} = (i_t, \dots, i_{t-k})$ from this point will be used.
To introduce the time delay $\tau$ the following embedding vector of the past states is constructed:
\begin{equation}\label{eq:embedding-vector}
  x_t^{k,\tau} = (x_t, x_{t-\tau}, x_{t-2\tau} \dots, x_{t-k\tau})
\end{equation}

However $\tau$ called also embedding delay in the most cases is set to be equal to 1, therefore is not used in the notation.
The order of markov process noted in equation \ref{eq:embedding-vector} as $k$ is often called embedding dimensions \cite{lee2012transfer}.

Finally $TE_{Y\to X}$ from random process $Y$  to random process $X$, with the corenspoding states $y_t$, $x_t$ and embedding dimensions $l$, $k$ is defined as:
\begin{equation}\label{eq:te}
  TE_{Y\to X}^{k,l} = \sum p(x_t,x_{t-1}^{(k)},y_{t-1}^{(l)}) \cdot log(\frac{p(x_t|x_{t-1}^{(k)},y_{t-1}^{(l)})}{p(x_t|x_{t-1}^{(k)})})
\end{equation}

Usually the following property is being used to simplify TE formula:
\begin{equation}
  p(A|B) = \frac{p(A,B)}{p(B)}
\end{equation}

So \ref{eq:te} can be also represented as:
\begin{equation}\label{eq:te-simplified}
  TE_{Y\to X}^{k,l} = \sum p(x_t,x_{t-1}^{(k)},y_{t-1}^{(l)}) \cdot log(\frac{p(x_t,x_{t-1}^{(k)},y_{t-1}^{(l)})p(x_{t-1}^{(k)})}{p(x_t,x_{t-1}^{(k)})p(x_{t-1}^{(k)},y_{t-1}^{(l)})})
\end{equation}

This way the calculation comes down to calculating only the joint probabilities.
From the original paper and other scientific publications the embedding dimensions $l$ and $k$ are usually set as $l=k=1$ (\cite{Schreiber2000};\cite{lee2012transfer}).

\subsection{Conditional Transfer Entropy and Conditional Joint Transfer Entropy}
Unfortunatly TE is often under or over estimated due to presense of the hidden interactions that rule the very nature of the analysed casuality.
Due to that conditioning has been proven as a necessary mean to achive the best accuracy in accessing the dynamics.
This effect is achived in Conditional Transfer Entropy (CTE) by introducing a third variable $Z$ (\cite{mehta2018directional}\cite{shahsavari2020estimating}).

Similarly to the definition \ref{eq:te} $CTE_{Y\to X|Z}$ is equal to:
\begin{equation}\label{eq:cte}
  CTE_{Y\to X|Z}^{k} = \sum p(x_t,x_{t-1}^{(k)},y_{t-1}^{(l)},z_{t-1}^{(k)}) \cdot log(\frac{p(x_t|x_{t-1}^{(k)},y_{t-1}^{(k)},z_{t-1}^{(k)})}{p(x_t|x_{t-1}^{(k)},z_{t-1}^{(k)})})
\end{equation}

There are scenarios where accessing the information transfer from two, or more, sources is needed.
In such cases, Joint Transfer Entropy (JTE) is the solution, or to include also conditioning, Conditional Joint Transfer Entropy (CJTE).
CJTE can be also used in the way, where the same signal is being used twice - as source and as condition ($Y=W$) - resulting in synergic impact of $X$ and $Y$ on $Z$, excluding past of $Y$ \cite{porta2015disentangling}.

\begin{equation}\label{eq:jte}
  JTE_{(X,Y)\to Z}^{k} = \sum p(z_t,z_{t-1}^{(k)},x_{t-1}^{(l)},y_{t-1}^{(k)}) \cdot log(\frac{p(z_t|z_{t-1}^{(k)},x_{t-1}^{(k)},y_{t-1}^{(k)})}{p(z_t|z_{t-1}^{(k)})})
\end{equation}

\begin{equation}\label{eq:cjte}
  CJTE_{(X,Y)\to Z|W}^{k} = \sum p(z_t,z_{t-1}^{(k)},x_{t-1}^{(k)},y_{t-1}^{(k)},w_{t-1}^{(k)}) \cdot log(\frac{p(z_t|z_{t-1}^{(k)},x_{t-1}^{(k)},y_{t-1}^{(k)},w_{t-1}^{(k)})}{p(z_t|z_{t-1}^{(k)},w_{t-1}^{(k)})})
\end{equation}

\subsection{Probability Estimation Techniques}
One of the biggest challanges in TE computation is estimation of the probabilities of the transition states, since their real probability is unknown (\cite{genccaga2018transfer};\cite{rozo2021benchmarking};\cite{lee2012transfer}).

There are numerous techniques that are being used for this purpose, each with its own benefits and drawbacks, thus the choice is not trivial.
However, Darbelly-Vajda Partitioning (DVP) algorithm seems to be a reliable solution, with no tunning needed (\cite{lee2012transfer};\cite{rozo2021benchmarking}).

DVP performs adaptive, non-uniform partition of the space defined by time series of the observations.
Taking TE as an example firstly we create embedded vectors $x_t$, $x_{t-1}$, $y_{t-1}$ from $X$ and $Y$, then we rank transform them into $u_t$, $u_{t-1}$, $v_{t-1}$ respectively, keeping the original length $N$ of the signals.
This results in $d=3$ standing for three-dimensional space for DVP calculation.
Inside the algorithm, firstly we divide the space into $2^d=8$ $d$-dimensional, equal cubes, with boundarues at the mid-points.
The chia test then is performed to test whether the distribution of the data points across all the created cubes is uniform.
If the null hypothesis is rejected, then algorithm runs in recursion in each of the boxes, else the current boxes are takes as one partition.
As the result, each box contains positive number of points, and by counting ammount of points within boundaries and dividinng it by $N$, the joint probability is estimated \cite{dvp}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{figs/dvp.png}
    \caption{DVP example}
    \label{fig:dvp-example}
\end{figure}




