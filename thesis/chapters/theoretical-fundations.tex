\chapter{Theoretical Foundations}\label{ch:theoretical-fundations}

\section{Baroreflex}\label{sec:baroreflex}
%TODO: Czemu te sygnały, metody (z fizjologii) i co2?
% feedfoward feedbackward - jako dwa rodzaje przenoszenia z abp na hp i co badamy
% niska f oddechu powoduje zmniejszona aktywnosc sns i nie ma wtedy zmiennosci oddechu podczas pomiaru - kontrola warunku
% modulowanie wspolczolnego oddechu, wykluczi
% miecej metod mierzenia baroreflexu, (sekwencyjna, spektralna, kros korelacyjna)
% nieinwazyjnosc jest z mierzenia abp
% jak wolne oddychanie wiaze sie z zwiekszona aktywnoscia ukladu wspolczolnego.
% Niska czestotliwosc oddechu wiaze sie z niska wartoscia etco2, wiec bez jego uwzglednienia mozemy dostac zle wyniki.

\subsection{What is a Baroreflex?}
The ANS is highly modulated by the baroreflex, a critical homeostatic mechanism.
The baroreflex is a negative feedback control system that maintains ABP despite fluctuations in the RR interval, i.e., the heart period (HP).
Baroreceptors are the crucial component of this system, placed in the carotid sinuses and the aortic arch, as well as the chambers of the heart and vena cavae \cite{heartmetronome}.

When active, baroreceptors fire action potenrials, with frequency corenspoding to the ABP level.
This modulates ANS by activation of the PNS and inhibitation of th SNS, causing reduced peripheral resistance and decrease in HP with contractility, leading to ABP reduction.
In the oposite direction if SNS is activated, and vagal outflow inhibited the baroreflex increases ABP.

\subsection{Introduction to BRS}\label{subsec:brs}
This regulatory process, where ABP and HP influence each other, cause nonlinear (sigmoidal) relationship between them.
Disorder of this relationship is related to pathological states, imparied regulatory capacity and aging (\cite{heartmetronome};\cite{la1995baroreflex};\cite{baroreflexcurve}).

The primary function of the baroreflex is quantified by its sensitivity (BRS) or gain. 
BRS is defined as the quantitative relationship between the stimulus (input), typically the change in Systolic Arterial Pressure (SAP), and the resulting reflex response (output), the change in HP (RR interval). 
This index represents the neural gain of the ANS's rapid control system for maintaining blood pressure \cite{baroreflexcurve}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/Fitting-different-Boltzmann-sigmoidal-functions-to-real-cardiac-baroreflex-data.png}
    \caption{Baroreflex curve in a healthy subject \cite{baroreflexcurve}}
    \label{fig:baroreflexcurve}
\end{figure}

While BRS is conceptually a measure of the neural feedback mechanism (SAP $\to$ HP), the actual observed signal interaction is a closed-loop system, with present feedfoward mechanism, where HP also mechanically affects SAP via the Frank–Starling and Windkessel effects \cite{javorka2017causal}. 

\subsection{The Influence of Respiration and CO$_2$}
There are many factors influencing the baroreflex response, such as ventricular mechanoreceptors, chemoreceptors, and humoral substances. 
Respiration is a particularly critical physiological factor, since it continously affects ANS and baroreflex by \textit{"respiratory gate"} phenomenon.
It suggests that at every breath, respiratory sinus arrhythmia (RSA) is generated cauisng vagus nerve activity inhibition during inhale, shortering HP, and during expiration the opposite effect takes place [\cite{la1995baroreflex};\cite{russo2017physiological};\cite{la2008baroreflex}].

For measurement standarization and to minimize respiratory confounding effect, controlled breathing protocols are a common solution. 
However such protocols carry the risk that too low, or too high level of CO$_2$ in subjects will be present, affecting chemoreceptors, which may alter BRS estimation result.
Therefore, end-tidal CO$_2$ (ETCO$_2$) level monitoring is essential to assure the correctness of the analysis [\cite{debeck2010heart};\cite{Uryga_2024}].

\subsection{BRS Measurement Methods}
There are numerous ways of BRS measurement, each with its own benefits and drawbacks.

The oldest one, called also pharmacological method, involced injection of vasoconstrictive agent to induce ABP increase.
This forces cardiovascular system to operate over a wide range of pressures, allowing to obtain data points necessary to fit the complete baroreflex curve.
While highly accurate, this method is highly invasive and not feasible for long-term monitoring or large-scale application \cite{la1995baroreflex}.

Further studies were trying to asses the BRS in non-invasive way using classical time series analysis techniques from ABP and HP recordings.
These techniuques included both time and spectral domain linear coupling quantivication from cross-corelation or coherence functions.
Nonlinear coupling could be acquired by mutual information, but all of these methods were suffering from the presence of the bidirectional coupling between SAP and HP described in subsection \ref{subsec:brs} \cite{javorka2017causal}.

Modern apporaches focus on accessing the directional effect.
Two methods with their extensions are often cosidered - granger casuality (GC) and TE.
Studies suggest that these methods are equivalent to each other, under gaussian assumptions, however in case of non-linear dependencies, TE is found to be superior [\cite{javorka2017causal};\cite{lee2012transfer};\cite{barnett2009granger}].


\section{Information Theory}\label{sec:information-theory}
In 1948 an article titled: \textit{"A Mathematical Theory of Communication"} was proposed by Shannon, giving raise to a new discipline called Information Theory \cite{Shannon1948}.

Shannon's main objective was to establish a quantitative measure of information independent of its meaning.
From this foundation, his work provided the methodology to calculate two fundamental limits: the theoretical maximum compression of a message (measured in bits) without losing information it conveys, and the noisy channel capacity, which is the maximum rate at which information can be transmitted at nearly zero error.

The theory contributed fundamentally to fields like statistical physics, computer science, and mathematical statistics, and was also found useful in diverse areas such as economics, quantum mechanics, and in the study of human physiology (\cite{Cover2006};\cite{lossofcomplexity}).

\subsection{Shannon Entropy}
Shannon proposed his own definition of Entropy (called also Shannon Entropy), different from the one in the thermodynamics.

In Information Theory, The Entropy $H(X)$ of a discreate random variable $X$ and propability mass function $p(x) = Pr\{X = x\}, x \in X$ is defined as:
\begin{equation}\label{eq:entropy}
    H(X) = -\sum_{x\in X} p(x) \cdot log(p(x))
\end{equation}
For convenience $p(x)$ notation is used instead of $p_X(x)$, and $log$ over $log_2$.
Thus, $p(x)$ and $p(y)$ refer to two different random variables and are equivalent to $p_X(x)$ and $p_Y(y)$.

The base of $log$ in the formula denotes the unit in which Entropy is calculated, where base 2 stands for bits. 
The resulting number can be interpreted in different ways, based on the use case.
From the information transfer perspective $H(X)$ will mean the maximum theoretical compression of $X$, where in the biological field it will rather be interpreted as total uncertrainity, ammount of information, or complexity of system $X$ (\cite{Cover2006};\cite{Shannon1948};\cite{lossofcomplexity}).

As an example, we can take tossing a fair coin 3 times, denoting its result as $X$. 
In Table \ref{tab:cointossing} we observe that we can have 8 possible outcomes $x$, each of equal probability $p(x)$ equal to $\frac{1}{8}$.
After putting this into formula \ref{eq:entropy} we have:
\begin{equation}
  H(X) = -\sum_{x=1}^{8} p(x) \cdot log(p(x)) = 3\text{ [bits]}
\end{equation}

\begin{table}[H]
\centering
\caption{All possible combinations of tossing a fair coin 3 times, where H and T denotes respectively heads and tails}
\label{tab:cointossing}
\begin{tabular}{|c|c|c|}
\hline
\textbf{No.} & \textbf{Coin Combination} & \textbf{Probability} \\
\hline
1 & HHH & $\frac{1}{8}$ \\
2 & HHT & $\frac{1}{8}$ \\
3 & HTH & $\frac{1}{8}$ \\
4 & HTT & $\frac{1}{8}$ \\
5 & THH & $\frac{1}{8}$ \\
6 & THT & $\frac{1}{8}$ \\
7 & TTH & $\frac{1}{8}$ \\
8 & TTT & $\frac{1}{8}$ \\
\hline
\end{tabular}
\end{table}

In terms of biological systems improved versions of Shannon Entropy are found to be more effective, such as Fuzzy, Sample, Distribution, or Multiscale Entropy(\cite{Costa2003};\cite{Castiglioni2023}).
Nevertheless the core idea of measuring the ammount of information in the system stays the same.

\subsection{Joint Entropy and Conditional Entropy}\label{subsec:je_ce}
The natural extension of Entropy will be calculating it for a pair of random variables $X$ and $Y$.

The Joint Entropy (JE) $H(X,Y)$ of a pair of discrete random variables ($X$, $Y$) with a joint distribution $p(x,y)$ is defined as:
\begin{equation}\label{eq:joint-entropy}
    H(X,Y) = -\sum p(x,y) \cdot log(p(x,y))
\end{equation}

For the simplification, the following notation was shorthanded with signle $\sum$ notation: $\sum_{x\in X}\sum_{y\in Y}$ and will be kept for the remaining definitions.

Instead of calculating Entropy for the joint propability distribution, we can define Entropy over the conditioning variable instead.

The Conditional Entropy (CE) $H(X|Y)$ if ($X$,$Y$) $\sim$ $p(x,y)$ is:
\begin{equation}\label{eq:conditional-entropy}
    H(X|Y) = -\sum p(x,y) \cdot log(p(x|y))
\end{equation}

The relation between CE  and JE  can be expressed using the following chain rule:

\begin{equation}\label{eq:conditional-entropy-chain}
    H(X|Y) = H(X,Y) - H(Y)
\end{equation}

The CE can be interpreted as a reduction of the uncertrainity of $X$ given $Y$, so the better $Y$ helps to predict $X$, the smaller $H(X|Y)$ is (\cite{Shannon1948};\cite{Cover2006}).

\subsection{Transfer entropy}
%TODO: DODAĆ KOD
To measure to which extent information is transfered from individual components in a dynamic and non-determenistic systems, TE was introduced as a model-free tool (\cite{Schreiber2000};\cite{wen2023kendall}). 

The dynamical structure is needed for TE calculation, rather than static probabilities, as it had place in the previous formulas.
This effect can be achived with the introduction of transition probabilities from Markov process.
The Markov random process $X$ of order $k$ has a property, that conditional probability of finding it in state $x_{t}$ at time $t$, is independent of the state $x_{t-k-1}$.
This fact can be expressed with the following equation:
\begin{equation}\label{eq:markov-chain}
  p(x_t|x_{t-1}, \dots, x_{t-k-1}) = p(x_t|x_{t-1}, \dots, x_{t-k})
\end{equation}

In other words - the conditional probability of finding $X$ in state $x_t$ only depends on $k$ last states.
The shorthand notation $x_t^{(k)} = (i_t, \dots, i_{t-k})$ from this point will be used.
To introduce the time delay $\tau$ the following embedding vector of the past states is constructed:
\begin{equation}\label{eq:embedding-vector}
  x_t^{k,\tau} = (x_t, x_{t-\tau}, x_{t-2\tau} \dots, x_{t-k\tau})
\end{equation}

However $\tau$ called also embedding delay in the most cases is set to be equal to 1, therefore is not used in the notation.
The order of markov process noted in equation \ref{eq:embedding-vector} as $k$ is often called embedding dimensions \cite{lee2012transfer}.

Finally $TE_{Y\to X}$ from random process $Y$  to random process $X$, with the corenspoding states $y_t$, $x_t$ and embedding dimensions $l$, $k$ is defined as:
\begin{equation}\label{eq:te}
  TE_{Y\to X}^{k,l} = \sum p(x_t,x_{t-1}^{(k)},y_{t-1}^{(l)}) \cdot log(\frac{p(x_t|x_{t-1}^{(k)},y_{t-1}^{(l)})}{p(x_t|x_{t-1}^{(k)})})
\end{equation}

Usually the following property is being used to simplify TE formula:
\begin{equation}
  p(A|B) = \frac{p(A,B)}{p(B)}
\end{equation}

So \ref{eq:te} can be also represented as:
\begin{equation}\label{eq:te-simplified}
  TE_{Y\to X}^{k,l} = \sum p(x_t,x_{t-1}^{(k)},y_{t-1}^{(l)}) \cdot log(\frac{p(x_t,x_{t-1}^{(k)},y_{t-1}^{(l)})p(x_{t-1}^{(k)})}{p(x_t,x_{t-1}^{(k)})p(x_{t-1}^{(k)},y_{t-1}^{(l)})})
\end{equation}

This way the calculation comes down to calculating only the joint probabilities.
From the original paper and other scientific publications the embedding dimensions $l$ and $k$ are usually set as $l=k=1$ (\cite{Schreiber2000};\cite{lee2012transfer}).

\subsection{Conditional Transfer Entropy and Conditional Joint Transfer Entropy}
Unfortunatly TE is often under or over estimated due to presense of the hidden interactions that rule the very nature of the analysed casuality.
Due to that conditioning has been proven as a necessary mean to achive the best accuracy in accessing the dynamics.
This effect is achived in Conditional Transfer Entropy (CTE) by introducing a third variable $Z$ (\cite{mehta2018directional}\cite{shahsavari2020estimating}).

Similarly to the definition \ref{eq:te} $CTE_{Y\to X|Z}$ is equal to:
\begin{equation}\label{eq:cte}
  CTE_{Y\to X|Z}^{k} = \sum p(x_t,x_{t-1}^{(k)},y_{t-1}^{(l)},z_{t-1}^{(k)}) \cdot log(\frac{p(x_t|x_{t-1}^{(k)},y_{t-1}^{(k)},z_{t-1}^{(k)})}{p(x_t|x_{t-1}^{(k)},z_{t-1}^{(k)})})
\end{equation}

There are scenarios where accessing the information transfer from two, or more, sources is needed.
In such cases, Joint Transfer Entropy (JTE) is the solution, or to include also conditioning, Conditional Joint Transfer Entropy (CJTE).
CJTE can be also used in the way, where the same signal is being used twice - as source and as condition ($Y=W$) - resulting in synergic impact of $X$ and $Y$ on $Z$, excluding past of $Y$ \cite{porta2015disentangling}.

\begin{equation}\label{eq:jte}
  JTE_{(X,Y)\to Z}^{k} = \sum p(z_t,z_{t-1}^{(k)},x_{t-1}^{(l)},y_{t-1}^{(k)}) \cdot log(\frac{p(z_t|z_{t-1}^{(k)},x_{t-1}^{(k)},y_{t-1}^{(k)})}{p(z_t|z_{t-1}^{(k)})})
\end{equation}

\begin{equation}\label{eq:cjte}
  CJTE_{(X,Y)\to Z|W}^{k} = \sum p(z_t,z_{t-1}^{(k)},x_{t-1}^{(k)},y_{t-1}^{(k)},w_{t-1}^{(k)}) \cdot log(\frac{p(z_t|z_{t-1}^{(k)},x_{t-1}^{(k)},y_{t-1}^{(k)},w_{t-1}^{(k)})}{p(z_t|z_{t-1}^{(k)},w_{t-1}^{(k)})})
\end{equation}

\subsection{Probability Estimation Techniques}
%TODO: moj pomysl
% zastosowane
% ta techinka wpływa mocno na wyniki
% listingi z kodem?
One of the biggest challanges in TE computation is estimation of the probabilities of the transition states, since their real probability is unknown (\cite{genccaga2018transfer};\cite{rozo2021benchmarking};\cite{lee2012transfer}).

There are numerous techniques that are being used for this purpose, each with its own benefits and drawbacks, thus the choice is not trivial.
However, Darbelly-Vajda Partitioning (DVP) algorithm seems to be a reliable solution, with no tunning needed (\cite{lee2012transfer};\cite{rozo2021benchmarking}).

DVP performs adaptive, non-uniform partition of the space defined by time series of the observations.
Taking TE as an example firstly we create embedded vectors $x_t$, $x_{t-1}$, $y_{t-1}$ from $X$ and $Y$, then we rank transform them into $u_t$, $u_{t-1}$, $v_{t-1}$ respectively, keeping the original length $N$ of the signals.
This results in $d=3$ standing for three-dimensional space for DVP calculation.
Inside the algorithm, firstly we divide the space into $2^d=8$ $d$-dimensional, equal cubes, with boundarues at the mid-points.
The chia test then is performed to test whether the distribution of the data points across all the created cubes is uniform.
If the null hypothesis is rejected, then algorithm runs in recursion in each of the boxes, else the current boxes are takes as one partition.
As the result, each box contains positive number of points, and by counting ammount of points within boundaries and dividinng it by $N$, the joint probability is estimated \cite{dvp}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{figs/dvp.png}
    \caption{DVP example}
    \label{fig:dvp-example}
\end{figure}




