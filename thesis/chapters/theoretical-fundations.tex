\chapter{Theoretical Foundations}\label{ch:theorfund}

\section{Information theory}
Mathematical treatment of the communication proposed by \cite{Shannon1948} where the definition of entropy was presented. Based on so called \textit{Shannon entropy} all the entropy--based solutions are built on.

\subsection{Shannon entropy}
The first and the simplest form of entropy definition.
The Shannon entropy of discrete random variable \textit{X} with a probability mass fucntion $p$ is defined as:
\begin{equation}\label{eq:entropy}
    H(X) = -\sum_{x\in X} p(x) \cdot log(p(x))
\end{equation}
Used \textit{log} notation denotes natural logarithm, and the outcome can be understood as ammount of information in nats. 
In case of logarithm of base 2 that would mean that the information is measured in bits and would be at the same time the data compression limit \cite{Cover2006}.

As an example of how it works, we can take tossing a fair coin 3 times. In Table \ref{tab:cointossing} we observe that tossing a coin 3 times can result in one of the 8 possible states, each of equal probability equal to $\frac{1}{16}$. After putting this into formula \ref{eq:entropy} we are getting:
\begin{equation}
    H(X) = -\sum_{i=1}^{8} p(i) \cdot log(p(i)) = -\sum_{i=1}^{8} \frac{1}{8} \cdot log(\frac{1}{8}) \approx 2.08\text{ nats}
\end{equation}
or for base 2 logarithm:
\begin{equation}
    H(X) = -\sum_{i=1}^{8} p(i) \cdot log_2(p(i)) = 3\text{ bits}
\end{equation}

Oryginally natural logarithms were used and so is done in this work. 

It is worth mentioning that for many cases in biological signals imporoved versions of Shannon entropy are commonly used, such as fuzzy, sample entropy \cite{Castiglioni2023}, or multiscale entropy which applies entropy to different time scales \cite{Costa2003}.
\begin{table}[H]
\centering
\caption{All possible combinations of tossing a fair coin 3 times, where H and T denotes respectively heads and tails}
\label{tab:cointossing}
\begin{tabular}{|c|c|c|}
\hline
\textbf{No.} & \textbf{Coin Combination} & \textbf{Probability p(x)} \\
\hline
1 & HHH & $\frac{1}{8}$ \\
2 & HHT & $\frac{1}{8}$ \\
3 & HTH & $\frac{1}{8}$ \\
4 & HTT & $\frac{1}{8}$ \\
5 & THH & $\frac{1}{8}$ \\
6 & THT & $\frac{1}{8}$ \\
7 & TTH & $\frac{1}{8}$ \\
8 & TTT & $\frac{1}{8}$ \\
\hline
\end{tabular}
\end{table}

\subsection{Joint entropy}
The slight modyfication of Shannon Entropy is Joint Entropy which is uncertainty included in the random vector $(X,Y)$ defined as:
\begin{equation}\label{eq:joint_entropy}
    H(X,Y) = -\sum_{x\in X}\sum_{y\in Y} p(x,y) \cdot log(p(x,y))
\end{equation}
As and example we can provide extension of previous experiment by tossing a coin 2 times with addition of rolling a dice once. The propability is in Table \ref{tab:cointossing_dice_2coins}. After applying formula \ref{eq:joint_entropy} we are getting:
\begin{equation}
    H(X,Y) = -\sum_{i=1}^{4}\sum_{j=1}^{6} p(i,j) \cdot log(p(i,j)) = -\sum_{i=1}^{4}\sum_{j=1}^{6} p(\frac{1}{24}) \cdot log(p(\frac{1}{24})) \approx 3.18\text{ nats}
\end{equation}
\begin{table}[H]
\centering
\caption{All possible combinations of tossing a fair coin 2 times and throwing a fair 6-sided die once, where H and T denote heads and tails}
\label{tab:cointossing_dice_2coins}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{No.} & \textbf{Coin Combination} & \textbf{Die Result} & \textbf{Probability $p(x,y)$} \\
\hline
1  & HH & 1 & $\frac{1}{24}$ \\
2  & HH & 2 & $\frac{1}{24}$ \\
3  & HH & 3 & $\frac{1}{24}$ \\
4  & HH & 4 & $\frac{1}{24}$ \\
5  & HH & 5 & $\frac{1}{24}$ \\
6  & HH & 6 & $\frac{1}{24}$ \\
7  & HT & 1 & $\frac{1}{24}$ \\
8  & HT & 2 & $\frac{1}{24}$ \\
9  & HT & 3 & $\frac{1}{24}$ \\
10 & HT & 4 & $\frac{1}{24}$ \\
11 & HT & 5 & $\frac{1}{24}$ \\
12 & HT & 6 & $\frac{1}{24}$ \\
13 & TH & 1 & $\frac{1}{24}$ \\
14 & TH & 2 & $\frac{1}{24}$ \\
15 & TH & 3 & $\frac{1}{24}$ \\
16 & TH & 4 & $\frac{1}{24}$ \\
17 & TH & 5 & $\frac{1}{24}$ \\
18 & TH & 6 & $\frac{1}{24}$ \\
19 & TT & 1 & $\frac{1}{24}$ \\
20 & TT & 2 & $\frac{1}{24}$ \\
21 & TT & 3 & $\frac{1}{24}$ \\
22 & TT & 4 & $\frac{1}{24}$ \\
23 & TT & 5 & $\frac{1}{24}$ \\
24 & TT & 6 & $\frac{1}{24}$ \\
\hline
\end{tabular}
\end{table}

\subsection{Conditional entropy}
Another important definition is Conditional Entropy (CE):

\begin{equation}\label{eq:conditional_entropy}
    H(X|Y) = -\sum_{x\in X}\sum_{y\in Y} p(x,y) \cdot log(\frac{p(x,y)}{p(y)})
\end{equation}

We can also define Conditional entropy with usage of  Joint and Shannon entropy using chain rule:

\begin{equation}\label{eq:conditional_entropy_chain}
    H(X|Y) = H(X,Y) - H(Y)
\end{equation}

We can consider it as a uncertrainity of $X$ given $Y$. It has also the following property:
\begin{equation}
    0 \leq H(X|Y) \leq H(X)
\end{equation}
The lower the value of Conditional Entropy, the better $Y$ reduces uncertainty of X. At $H(X|Y)=H(X)$, the variable $Y$ is irrelevant for the reduction in uncertainty.

\subsection{Transfer entropy}
Definition of Transfer Entropy (TE) was proposed for the first time in the article "Measuring Information Transfer" by \cite{Schreiber2000}. It is a model-free measure of the directional inflow from a time series $Y_t$ to a time series $X_t$.

TE does Markovian assumption that \textit{a discrete time stochastic process $\{X_t\}_t\in N$ is a Markov chain of order m when, for any $t>m$, the follwoing property holds}:
\begin{equation}\label{eq:markov-chain}
    P(X_t=x_t|X_{t-1} = x_{t-1}, \dots, X_1=x_1) =
\end{equation}
\begin{equation*}
     P(X_t=x_t|X_{t-1} = x_{t-1}, \dots, X_{t-m}=x_{t-m})
\end{equation*}

So, the future state of this process depends only on its past $m$ states. From the notation it also results in that TE will be considered only in discrete time.

The last thing to define TE is Embedding Vector. \textit{Let $\{U_t\}_{t \in \mathbb{Z}}$ be a time series. The embedding vector $U_t^{d,\tau}$ is the following random vector of past states of $U_t$}:
\begin{equation}\label{eq:embedding-vector}
    U_t^{d,\tau} = (U_t,U_{t-\tau},U_{t-2\tau},\dots,U_{t-(d-1)\tau})
\end{equation}

In our case the notation can be simplified to $U_t^{(d)}$ when $\tau = 1$. In literature, the parameter d is called the embedding dimension and $\tau$ is called the embedding delay.

Now we can finally define Transfer Entropy at time t, from $l^{th}$ order Markov process $Y_t$ to the $k^{th}$ order Markov process $X_t$. Simillary as in case of CE we can do it in both ways:
\begin{equation}\label{eq:te-analytical}
    TE_{Y\to X}^{k,l}(t) = \sum_{x_t,x_{t-1}^{(k)},y_{t-1}^{(l)}}p(x_t,x_{t-1}^{(k)},y_{t-1}^{(l)}) \cdot log(\frac{x_t|x_{t-1}^{(k)},y_{t-1}^{(l)}}{x_t|x_{t-1}^{(k)}})
\end{equation}
or
\begin{equation}\label{eq:te-nonanalytical}
    TE_{Y\to X}^{k,l}(t) = H(X_{t}|X_{t-1}^{(k)}) - H(X_t|X_{t-1}^{(k)},Y_{t-1}^{(m)}) 
\end{equation}

From equation \ref{eq:te-nonanalytical} we can understand it as a reduction in uncertainty of $X$ resolved by past values of $Y$ over past values of just $X$.

\subsection{Conditional joint transfer entropy}

\section{Estimation techniques}
\cite{genccaga2018transfer}\cite{lee2012transfer}\cite{rozo2021benchmarking}\cite{wen2023kendall}\cite{dvp}

\section{Autonomic nervous system}

